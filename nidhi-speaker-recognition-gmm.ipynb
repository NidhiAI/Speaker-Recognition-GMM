{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/NidhiAI/Speaker-Recognition-GMM","metadata":{"execution":{"iopub.status.busy":"2023-02-15T07:54:51.973252Z","iopub.execute_input":"2023-02-15T07:54:51.974166Z","iopub.status.idle":"2023-02-15T07:54:52.245971Z","shell.execute_reply.started":"2023-02-15T07:54:51.974131Z","shell.execute_reply":"2023-02-15T07:54:52.244670Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"fatal: destination path 'Speaker-Recognition-GMM' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install python_speech_features","metadata":{"execution":{"iopub.status.busy":"2023-02-15T07:48:45.300942Z","iopub.execute_input":"2023-02-15T07:48:45.301334Z","iopub.status.idle":"2023-02-15T07:48:57.652745Z","shell.execute_reply.started":"2023-02-15T07:48:45.301299Z","shell.execute_reply":"2023-02-15T07:48:57.651272Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting python_speech_features\n  Downloading python_speech_features-0.6.tar.gz (5.6 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: python_speech_features\n  Building wheel for python_speech_features (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for python_speech_features: filename=python_speech_features-0.6-py3-none-any.whl size=5888 sha256=fd6fa85c6e6b53914f3bf70ebad8b76e8af76b3340f9f170f70c6cf401e19363\n  Stored in directory: /root/.cache/pip/wheels/b0/0e/94/28cd6afa3cd5998a63eef99fe31777acd7d758f59cf24839eb\nSuccessfully built python_speech_features\nInstalling collected packages: python_speech_features\nSuccessfully installed python_speech_features-0.6\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn import preprocessing\nimport python_speech_features as mfcc\n\ndef calculate_delta(array):\n    \"\"\"Calculate and returns the delta of given feature vector matrix\"\"\"\n\n    rows,cols = array.shape\n    deltas = np.zeros((rows,20))\n    N = 2\n    for i in range(rows):\n        index = []\n        j = 1\n        while j <= N:\n            if i-j < 0:\n              first =0\n            else:\n              first = i-j\n            if i+j > rows-1:\n                second = rows-1\n            else:\n                second = i+j \n            index.append((second,first))\n            j+=1\n        deltas[i] = ( array[index[0][0]]-array[index[0][1]] + (2 * (array[index[1][0]]-array[index[1][1]])) ) / 10\n    return deltas\n\ndef extract_features(audio,rate):\n    \"\"\"extract 20 dim mfcc features from an audio, performs CMS and combines \n    delta to make it 40 dim feature vector\"\"\"    \n    \n    mfcc_feature = mfcc.mfcc(audio,rate, 0.025, 0.01,20,nfft = 1200, appendEnergy = True)    \n    mfcc_feature = preprocessing.scale(mfcc_feature)\n    delta = calculate_delta(mfcc_feature)\n    combined = np.hstack((mfcc_feature,delta)) \n    return combined","metadata":{"execution":{"iopub.status.busy":"2023-02-15T07:49:02.764274Z","iopub.execute_input":"2023-02-15T07:49:02.764635Z","iopub.status.idle":"2023-02-15T07:49:03.256331Z","shell.execute_reply.started":"2023-02-15T07:49:02.764604Z","shell.execute_reply":"2023-02-15T07:49:03.254556Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import _pickle as cPickle\nimport numpy as np\nfrom scipy.io.wavfile import read\nfrom sklearn.mixture import GaussianMixture as GMM\n#from featureextraction import extract_features\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#path to training data\nsource   =\"/kaggle/working/Speaker-Recognition-GMM/trainingData/\"\n\n#path to store trainged GMM models\ndest = \"/kaggle/working/Speaker-Recognition-GMM/speakerTrainedModelsGMM/\"\n\n#training data files names and folders  \ntrain_file = \"/kaggle/working/Speaker-Recognition-GMM/trainingDataPath.txt\"\nfile_paths = open(train_file,'r')\n\ncount = 1\n# Extracting features for each speaker (10 files per speakers)\nfeatures = np.asarray(())\nfor path in file_paths:    \n    path = path.strip()   \n    print (path)\n    \n    # read the audio\n    sr,audio = read(source+path)\n    \n    # extract 40 dimensional MFCC & delta MFCC features\n    vector   = extract_features(audio,sr)\n    \n    if features.size == 0:\n        features = vector\n    else:\n        features = np.vstack((features, vector))\n    # when features of 5 files of speaker are concatenated, then do model training\n\t# -> if count == 5: --> edited below\n    if count == 10:    \n        gmm = GMM(n_components = 5, covariance_type='diag',n_init = 3)\n        gmm.fit(features)\n        # dumping the trained gaussian model\n        picklefile = path.split(\"_\")[0]+\".gmm\"\n        cPickle.dump(gmm,open(dest + picklefile,'wb'))\n        print ('+ modeling completed for speaker:',picklefile,\" with data point = \",features.shape)   \n        features = np.asarray(())\n        count = 0\n    count = count + 1","metadata":{"execution":{"iopub.status.busy":"2023-02-15T07:52:14.522683Z","iopub.execute_input":"2023-02-15T07:52:14.523082Z","iopub.status.idle":"2023-02-15T07:52:21.020250Z","shell.execute_reply.started":"2023-02-15T07:52:14.523051Z","shell.execute_reply":"2023-02-15T07:52:21.017457Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Akshay_Kumar/Akshay_Kumar-1.wav\nAkshay_Kumar/Akshay_Kumar-2.wav\nAkshay_Kumar/Akshay_Kumar-3.wav\nAkshay_Kumar/Akshay_Kumar-4.wav\nAkshay_Kumar/Akshay_Kumar-5.wav\nAkshay_Kumar/Akshay_Kumar-6.wav\nAkshay_Kumar/Akshay_Kumar-7.wav\nAkshay_Kumar/Akshay_Kumar-8.wav\nAkshay_Kumar/Akshay_Kumar-9.wav\nAkshay_Kumar/Akshay_Kumar-10.wav\n+ modeling completed for speaker: Akshay.gmm  with data point =  (5694, 40)\nIrfan_khan/Irfan_khan-1.wav\nIrfan_khan/Irfan_khan-2.wav\nIrfan_khan/Irfan_khan-3.wav\nIrfan_khan/Irfan_khan-4.wav\nIrfan_khan/Irfan_khan-5.wav\nIrfan_khan/Irfan_khan-6.wav\nIrfan_khan/Irfan_khan-7.wav\nIrfan_khan/Irfan_khan-8.wav\nIrfan_khan/Irfan_khan-9.wav\nIrfan_khan/Irfan_khan-10.wav\n+ modeling completed for speaker: Irfan.gmm  with data point =  (5862, 40)\nJohn_Abraham/John_Abraham-1.wav\nJohn_Abraham/John_Abraham-2.wav\nJohn_Abraham/John_Abraham-3.wav\nJohn_Abraham/John_Abraham-4.wav\nJohn_Abraham/John_Abraham-5.wav\nJohn_Abraham/John_Abraham-6.wav\nJohn_Abraham/John_Abraham-7.wav\nJohn_Abraham/John_Abraham-8.wav\nJohn_Abraham/John_Abraham-9.wav\nJohn_Abraham/John_Abraham-10.wav\n+ modeling completed for speaker: John.gmm  with data point =  (4786, 40)\nParineeti_Chopra/Parineeti_Chopra-1.wav\nParineeti_Chopra/Parineeti_Chopra-2.wav\nParineeti_Chopra/Parineeti_Chopra-3.wav\nParineeti_Chopra/Parineeti_Chopra-4.wav\nParineeti_Chopra/Parineeti_Chopra-5.wav\nParineeti_Chopra/Parineeti_Chopra-6.wav\nParineeti_Chopra/Parineeti_Chopra-7.wav\nParineeti_Chopra/Parineeti_Chopra-8.wav\nParineeti_Chopra/Parineeti_Chopra-9.wav\nParineeti_Chopra/Parineeti_Chopra-10.wav\n+ modeling completed for speaker: Parineeti.gmm  with data point =  (6014, 40)\nKangana_Ranaut/Kangana_Ranaut-1.wav\nKangana_Ranaut/Kangana_Ranaut-2.wav\nKangana_Ranaut/Kangana_Ranaut-3.wav\nKangana_Ranaut/Kangana_Ranaut-4.wav\nKangana_Ranaut/Kangana_Ranaut-5.wav\nKangana_Ranaut/Kangana_Ranaut-6.wav\nKangana_Ranaut/Kangana_Ranaut-7.wav\nKangana_Ranaut/Kangana_Ranaut-8.wav\nKangana_Ranaut/Kangana_Ranaut-9.wav\nKangana_Ranaut/Kangana_Ranaut-10.wav\n+ modeling completed for speaker: Kangana.gmm  with data point =  (6098, 40)\nVidya_Balan/Vidya_Balan-1.wav\nVidya_Balan/Vidya_Balan-2.wav\nVidya_Balan/Vidya_Balan-3.wav\nVidya_Balan/Vidya_Balan-4.wav\nVidya_Balan/Vidya_Balan-5.wav\nVidya_Balan/Vidya_Balan-6.wav\nVidya_Balan/Vidya_Balan-7.wav\nVidya_Balan/Vidya_Balan-8.wav\nVidya_Balan/Vidya_Balan-9.wav\nVidya_Balan/Vidya_Balan-10.wav\n+ modeling completed for speaker: Vidya.gmm  with data point =  (6146, 40)\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/2634486367.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# read the audio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# extract 40 dimensional MFCC & delta MFCC features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/scipy/io/wavfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(filename, mmap)\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0mmmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/kaggle/working/Speaker-Recognition-GMM/trainingData/'"],"ename":"IsADirectoryError","evalue":"[Errno 21] Is a directory: '/kaggle/working/Speaker-Recognition-GMM/trainingData/'","output_type":"error"}]},{"cell_type":"code","source":"import os\nimport _pickle as cPickle\nimport numpy as np\nfrom scipy.io.wavfile import read\n#from featureextraction import extract_features\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport time\n\n#path to get test data files name\ntest_file = \"/kaggle/working/Speaker-Recognition-GMM/testDataPath.txt\"       \nfile_paths = open(test_file,'r')\n\n#path to read test data wav files\nsource   = \"/kaggle/working/Speaker-Recognition-GMM/testData/\"   \n\n#path where training speakers GMM modles are saved\nmodelpath =  \"/kaggle/working/Speaker-Recognition-GMM/speakerTrainedModelsGMM/\"\n##\"Trained_Speech_Models/\"\n\ngmm_files = [os.path.join(modelpath,fname) for fname in \n              os.listdir(modelpath) if fname.endswith('.gmm')]\n\n#Load the GMM Gaussian Models\nmodels    = [cPickle.load(open(fname,'rb')) for fname in gmm_files]\nspeakers   = [fname.split(\"/\")[-1].split(\".gmm\")[0] for fname \n              in gmm_files]\n\nerror = 0\ntotal_sample = 0.0\n\nprint(\"Press '1' for checking a single Audio or Press '0' for testing a complete set of audio with Accuracy?\")\ntake=int(input().strip())\nif take == 1:\n    print (\"Enter the File name from the sample with .wav notation :\")\n    path =input().strip()\n    print ((\"Testing Audio : \",path))\n    sr,audio = read(source + path)\n    vector   = extract_features(audio,sr)\n    \n    log_likelihood = np.zeros(len(models)) \n    \n    for i in range(len(models)):\n        gmm    = models[i]  #checking with each model one by one\n        scores = np.array(gmm.score(vector))\n        log_likelihood[i] = scores.sum()\n    \n    winner = np.argmax(log_likelihood)\n    print (\"\\tThe person in the given audio sample is detected as - \", speakers[winner])\n\n    time.sleep(1.0)\nelif take == 0:\n    test_file = \"/kaggle/working/Speaker-Recognition-GMM/testDataPath.txt\"         \n    file_paths = open(test_file,'r')\n    # Read the test directory and get the list of test audio files \n    for path in file_paths:   \n        total_sample+= 1.0\n        path=path.strip()\n        print(\"Testing Audio : \", path)\n        sr,audio = read(source + path)\n        vector   = extract_features(audio,sr)\n        log_likelihood = np.zeros(len(models)) \n        for i in range(len(models)):\n            gmm    = models[i]  #checking with each model one by one\n            scores = np.array(gmm.score(vector))\n            log_likelihood[i] = scores.sum()\n        winner=np.argmax(log_likelihood)\n        print (\"\\tdetected as - \", speakers[winner])\n        checker_name = path.split(\"_\")[0]\n        if speakers[winner] != checker_name:\n            error += 1\n        time.sleep(1.0)\n    print (error, total_sample)\n    accuracy = ((total_sample - error) / total_sample) * 100\n\n    print (\"The Accuracy Percentage for the current testing Performance with MFCC + GMM is : \", accuracy, \"%\")\n\n\nprint (\"Speaker Identified Successfully\")","metadata":{"execution":{"iopub.status.busy":"2023-02-15T07:59:10.578914Z","iopub.execute_input":"2023-02-15T07:59:10.579316Z","iopub.status.idle":"2023-02-15T07:59:16.527974Z","shell.execute_reply.started":"2023-02-15T07:59:10.579290Z","shell.execute_reply":"2023-02-15T07:59:16.526595Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Press '1' for checking a single Audio or Press '0' for testing a complete set of audio with Accuracy?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 1\n"},{"name":"stdout","text":"Enter the File name from the sample with .wav notation :\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" Irfak-11.wav\n"},{"name":"stdout","text":"('Testing Audio : ', 'Irfak-11.wav')\n\tThe person in the given audio sample is detected as -  Irfan\nSpeaker Identified Successfully\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}